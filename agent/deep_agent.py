"""Deep Agent - MVP ç‰ˆæœ¬ (Vertex AI + LangChain)"""
import logging
import os
from datetime import datetime
from typing import Optional, List, Callable, Dict, Any

from deepagents import create_deep_agent
from agent.models import AnthropicModelProvider
from agent.prompts import get_system_prompt
from deepagents.backends import FilesystemBackend
from agent.chunk_parser import ChunkParser
from langchain_core.messages import BaseMessage

# === è¼‰å…¥ä¸¦è¨»å†Šæ‰€æœ‰ tools å’Œ subagents ===
# é€™äº› import æœƒè§¸ç™¼ @register_tool å’Œ register_subagent
import agent.tools  # noqa: F401
import agent.subagents  # noqa: F401

# å¾ registry å–å¾—
from agent.registry import get_all_tools, get_all_subagents

logger = logging.getLogger(__name__)

from langchain.agents.middleware import SummarizationMiddleware

# é è¨­æŠ€èƒ½ç›®éŒ„ï¼ˆç›¸å°æ–¼ backend rootï¼‰
DEFAULT_SKILLS = ["/workspace/skills/"]


class RefactorAgent:
    def __init__(
        self,
        model=None,
        verbose: bool = True,
        stop_check_callback=None,
        postgres_url: Optional[str] = None,
        tools: Optional[List[Callable]] = None,
        skills: Optional[List[str]] = None,
        subagents: Optional[List[Dict[str, Any]]] = None,
        enable_code_execution: bool = True,
    ):
        """åˆå§‹åŒ– RefactorAgent

        Args:
            model: LLM æ¨¡å‹å¯¦ä¾‹
            verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°çš„ chunk è§£æè³‡è¨Š
            stop_check_callback: å¯é¸çš„åœæ­¢æª¢æŸ¥å›èª¿å‡½æ•¸ï¼Œè¿”å› True è¡¨ç¤ºæ‡‰è©²åœæ­¢
            postgres_url: PostgreSQL é€£ç·š URL (å¿…å¡«ï¼Œç”¨æ–¼æœƒè©±æŒä¹…åŒ–)
            tools: é¡å¤–çš„è‡ªå®šç¾©å·¥å…·åˆ—è¡¨ï¼ˆæœƒèˆ‡é è¨­å·¥å…·åˆä½µï¼‰
            skills: æŠ€èƒ½ç›®éŒ„åˆ—è¡¨ï¼ˆç›¸å°æ–¼ backend rootï¼‰
            subagents: è‡ªå®šç¾© subagents åˆ—è¡¨ï¼ˆæœƒèˆ‡é è¨­ subagents åˆä½µï¼‰
            enable_code_execution: æ˜¯å¦å•Ÿç”¨ç¨‹å¼ç¢¼åŸ·è¡Œå·¥å…·ï¼ˆé è¨­ Trueï¼‰

        Raises:
            ValueError: ç•¶ postgres_url æœªæä¾›æ™‚
            RuntimeError: ç•¶ PostgreSQL åˆå§‹åŒ–å¤±æ•—æ™‚
        """
        self.model = model
        self.verbose = verbose
        self.root_dir = "/"
        self.stop_check_callback = stop_check_callback
        self.enable_code_execution = enable_code_execution

        # æª¢æŸ¥ postgres_url å¿…å¡«
        if not postgres_url:
            raise ValueError(
                "PostgreSQL URL is required for persistence. "
                "Please set POSTGRES_URL environment variable."
            )
        self.postgres_url = postgres_url

        # è¨­å®šå·¥å…·ï¼ˆå¾ registry å–å¾—ï¼‰
        self.tools = []
        if enable_code_execution:
            self.tools.extend(get_all_tools())
        if tools:
            self.tools.extend(tools)

        # è¨­å®šæŠ€èƒ½
        self.skills = skills if skills is not None else DEFAULT_SKILLS

        # è¨­å®š subagentsï¼ˆå¾ registry å–å¾—ï¼‰
        self.subagents = list(get_all_subagents())
        if subagents:
            self.subagents.extend(subagents)

        # åˆå§‹åŒ–æŒä¹…åŒ–å¾Œç«¯ï¼ˆå¿…é ˆæˆåŠŸï¼‰
        self._setup_persistence()
        self._agent_init()

    def _setup_persistence(self):
        """è¨­ç½® PostgreSQL æŒä¹…åŒ–å¾Œç«¯ï¼ˆå¿…é ˆæˆåŠŸï¼Œä¸å…è¨± fallbackï¼‰

        åƒè€ƒ: https://docs.langchain.com/oss/python/langgraph/add-memory

        Raises:
            RuntimeError: ç•¶ç¼ºå°‘ä¾è³´å¥—ä»¶æˆ– PostgreSQL é€£æ¥å¤±æ•—æ™‚
        """
        try:
            from langgraph.checkpoint.postgres import PostgresSaver
            from langgraph.store.postgres import PostgresStore
            import psycopg

            logger.info(f"åˆå§‹åŒ– PostgreSQL æŒä¹…åŒ–: {self.postgres_url}")

            # å»ºç«‹ä¸¦ä¿æŒ PostgreSQL é€£æ¥
            # æ³¨æ„ï¼šé€™å€‹é€£æ¥æœƒåœ¨ Agent å¯¦ä¾‹çš„ç”Ÿå‘½é€±æœŸå…§ä¿æŒé–‹å•Ÿ
            self._pg_conn = psycopg.connect(
                self.postgres_url,
                autocommit=True,
                prepare_threshold=0,
            )

            # åˆå§‹åŒ– checkpointer
            self.checkpointer = PostgresSaver(self._pg_conn)
            self.checkpointer.setup()  # âš ï¸ å¿…é ˆå‘¼å« setup() å»ºç«‹è¡¨æ ¼ï¼ˆé¦–æ¬¡ä½¿ç”¨ï¼‰
            logger.info("âœ… PostgresSaver åˆå§‹åŒ–æˆåŠŸ")

            # åˆå§‹åŒ– storeï¼ˆç”¨æ–¼é•·æœŸè¨˜æ†¶ï¼‰
            self.store = PostgresStore(self._pg_conn)
            self.store.setup()  # âš ï¸ å¿…é ˆå‘¼å« setup() å»ºç«‹è¡¨æ ¼ï¼ˆé¦–æ¬¡ä½¿ç”¨ï¼‰
            logger.info("âœ… PostgresStore åˆå§‹åŒ–æˆåŠŸ")

        except ImportError as e:
            logger.error(f"âŒ ç¼ºå°‘ PostgreSQL ä¾è³´å¥—ä»¶: {e}")
            raise RuntimeError(
                "PostgreSQL persistence dependencies not installed. "
                "Please run: pip install langgraph-checkpoint-postgres psycopg[binary]"
            ) from e
        except Exception as e:
            logger.error(f"âŒ PostgreSQL åˆå§‹åŒ–å¤±æ•—: {e}")
            raise RuntimeError(
                f"Failed to initialize PostgreSQL persistence: {e}. "
                "Please check POSTGRES_URL and ensure PostgreSQL is running."
            ) from e

    def _agent_init(self):
        if not self.model:
            raise ValueError("model is not set")

        # è¨˜éŒ„å·¥å…·ã€æŠ€èƒ½å’Œ subagents é…ç½®
        tool_names = [t.__name__ for t in self.tools]
        subagent_names = [s["name"] for s in self.subagents]
        logger.info(
            f"åˆå§‹åŒ– Agent - å·¥å…·: {tool_names}, "
            f"æŠ€èƒ½ç›®éŒ„: {self.skills}, "
            f"Subagents: {subagent_names}, "
            f"æŒä¹…åŒ–: PostgreSQL (checkpointer + store)"
        )

        # middleware ç”± create_deep_agent åœ¨å•Ÿç”¨ checkpointer æ™‚è‡ªå‹•ç®¡ç†
        middleware = []

        self.agent = create_deep_agent(
            model=self.model,
            memory=[
                f"{self.root_dir}memory/AGENTS.md",
            ],
            tools=self.tools,
            skills=self.skills,
            subagents=self.subagents,
            backend=FilesystemBackend(
                root_dir=self.root_dir,
                virtual_mode=True
            ),
            system_prompt=get_system_prompt("autonomous_v3"),
            checkpointer=self.checkpointer,
            store=self.store,
            middleware=middleware if middleware else None,
        )

    def _content_to_text(self, content: Any) -> str:
        """å°‡ message content è½‰ç‚ºå¯é¡¯ç¤ºçš„æ–‡å­—"""
        if content is None:
            return ""
        if isinstance(content, str):
            return content
        if isinstance(content, list):
            parts: List[str] = []
            for item in content:
                if isinstance(item, str):
                    parts.append(item)
                elif isinstance(item, dict):
                    if "text" in item and isinstance(item["text"], str):
                        parts.append(item["text"])
                    elif "content" in item and isinstance(item["content"], str):
                        parts.append(item["content"])
                else:
                    parts.append(str(item))
            return "".join(parts)
        return str(content)

    def _normalize_messages(
        self,
        messages: List[BaseMessage],
        base_timestamp: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """å°‡ LangChain messages è½‰ç‚ºå‰ç«¯å¯ç”¨æ ¼å¼"""
        if base_timestamp is not None and not isinstance(base_timestamp, str):
            timestamp = str(base_timestamp)
        else:
            timestamp = base_timestamp or datetime.utcnow().isoformat()
        tool_outputs: Dict[str, Dict[str, Any]] = {}
        for msg in messages:
            if getattr(msg, "type", None) == "tool":
                tool_call_id = getattr(msg, "tool_call_id", None)
                if tool_call_id:
                    tool_outputs[tool_call_id] = {
                        "content": self._content_to_text(getattr(msg, "content", "")),
                        "name": getattr(msg, "name", None),
                        "id": getattr(msg, "id", None),
                    }

        normalized: List[Dict[str, Any]] = []
        used_tool_calls = set()
        for idx, msg in enumerate(messages):
            msg_type = getattr(msg, "type", None)

            if msg_type == "tool":
                tool_call_id = getattr(msg, "tool_call_id", None)
                if tool_call_id in used_tool_calls:
                    continue
                if tool_call_id:
                    used_tool_calls.add(tool_call_id)
                normalized.append(
                    {
                        "id": getattr(msg, "id", None) or f"tool-{idx}",
                        "role": "tool",
                        "content": "",
                        "timestamp": timestamp,
                        "toolName": getattr(msg, "name", None),
                        "toolCallId": tool_call_id,
                        "toolInput": None,
                        "toolOutput": self._content_to_text(getattr(msg, "content", "")),
                    }
                )
                continue

            role = {
                "human": "user",
                "ai": "assistant",
                "system": "system",
            }.get(msg_type, "assistant")

            normalized.append(
                {
                    "id": getattr(msg, "id", None) or f"{role}-{idx}",
                    "role": role,
                    "content": self._content_to_text(getattr(msg, "content", "")),
                    "timestamp": timestamp,
                }
            )

            if msg_type == "ai":
                tool_calls = getattr(msg, "tool_calls", None) or []
                for call in tool_calls:
                    tool_call_id = call.get("id") or f"tool-{idx}-{len(used_tool_calls)}"
                    used_tool_calls.add(tool_call_id)
                    output = tool_outputs.get(tool_call_id)
                    normalized.append(
                        {
                            "id": f"tool-{tool_call_id}",
                            "role": "tool",
                            "content": "",
                            "timestamp": timestamp,
                            "toolName": call.get("name"),
                            "toolCallId": tool_call_id,
                            "toolInput": call.get("args") or {},
                            "toolOutput": output.get("content") if output else None,
                        }
                    )

        return normalized

    def get_thread_history(
        self,
        thread_id: str,
        limit: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """å–å¾—æŒ‡å®š thread çš„èŠå¤©æ­·å²"""
        # ç°¡åŒ–æª¢æŸ¥ï¼ˆå› ç‚º __init__ å·²ç¢ºä¿ checkpointer å¿…å®šå­˜åœ¨ï¼‰
        assert self.checkpointer is not None, "Checkpointer should be initialized"

        config = {"configurable": {"thread_id": thread_id}}
        snapshot = self.agent.get_state(config)
        messages = snapshot.values.get("messages", []) if snapshot else []
        normalized = self._normalize_messages(messages, snapshot.created_at if snapshot else None)
        if limit and limit > 0:
            return normalized[-limit:]
        return normalized

    def run(
        self,
        user_message: str = "æª¢è¦–æˆ‘çš„å°ˆæ¡ˆè³‡æ–™å¤¾çµæ§‹",
        event_callback=None,
        thread_id: Optional[str] = None,
    ):
        """åŸ·è¡Œ Agent ä¸¦ä½¿ç”¨ ChunkParser è§£æä¸²æµè¼¸å‡º

        Args:
            user_message: ä½¿ç”¨è€…è¨Šæ¯
            event_callback: å¯é¸çš„å›èª¿å‡½æ•¸ï¼Œç”¨æ–¼è™•ç†æ¯å€‹è§£æäº‹ä»¶
                          å‡½æ•¸ç°½å: callback(event_type: str, data: dict)
            thread_id: å°è©±ç·šç¨‹ IDï¼Œç”¨æ–¼å¤šè¼ªå°è©±æŒä¹…åŒ–
        """
        # åˆå§‹åŒ– ChunkParserï¼ˆå‚³å…¥ callbackï¼‰
        parser = ChunkParser(verbose=self.verbose, event_callback=event_callback)

        print(f"\n{'='*60}", flush=True)
        print(f"ğŸš€ é–‹å§‹åŸ·è¡Œ Agent", flush=True)
        if thread_id:
            print(f"ğŸ“ Thread ID: {thread_id}", flush=True)
        print(f"{'='*60}\n", flush=True)
        print(f"ğŸ“ User Message: {user_message}\n", flush=True)
        print(f"{'â”€'*60}", flush=True)
        print(f"ğŸ’¬ AI Response:\n", flush=True)

        # è¨­ç½®é…ç½®ï¼ˆåŒ…å« thread_idï¼‰
        config = {}
        if thread_id:
            config = {"configurable": {"thread_id": thread_id}}

        # ä¸²æµåŸ·è¡Œ
        result = self.agent.stream(
            {
                "messages": [
                    {"role": "user", "content": user_message}
                ]
            },
            config=config,
        )

        # ä½¿ç”¨ ChunkParser è§£ææ¯å€‹ chunk
        try:
            for chunk in result:
                # æª¢æŸ¥æ˜¯å¦æ‡‰è©²åœæ­¢
                if self.stop_check_callback and self.stop_check_callback():
                    print(f"\n{'='*60}", flush=True)
                    print(f"â¹ï¸  æª¢æ¸¬åˆ°åœæ­¢ä¿¡è™Ÿï¼Œä¸­æ–· Agent åŸ·è¡Œ", flush=True)
                    print(f"{'='*60}\n", flush=True)
                    raise KeyboardInterrupt("Agent stopped by user")

                parser.parse(chunk)
        except KeyboardInterrupt:
            print(f"\n{'='*60}", flush=True)
            print(f"â¹ï¸  Agent åŸ·è¡Œå·²è¢«ä¸­æ–·", flush=True)
            print(f"{'='*60}\n", flush=True)
            raise

        # é¡¯ç¤ºç¸½çµ
        parser.print_summary()


if __name__ == "__main__":
    # åˆå§‹åŒ– LLM (æ ¹æ“šç’°å¢ƒè®Šæ•¸ LLM_PROVIDER è‡ªå‹•é¸æ“‡)
    provider = AnthropicModelProvider()
    model = provider.get_model()

    # å‰µå»º Agent (verbose=True æœƒé¡¯ç¤ºè©³ç´°çš„ token usage, tool calls ç­‰è³‡è¨Š)
    agent = RefactorAgent(model, verbose=True)
    message = """
    æª¢è¦–æˆ‘çš„è³‡æ–™å¤¾çµæ§‹ï¼Œä¸¦æ•´ç†ä¸€å€‹å°‡æ­¤å°ˆæ¡ˆé‡æ§‹æˆ typescript çš„è¨ˆç•«ï¼Œä¸¦å°‡æª”æ¡ˆå¯«å…¥ ./memory/plan.md æª”æ¡ˆ
    """
    # åŸ·è¡Œ Agent
    agent.run(user_message=message)
